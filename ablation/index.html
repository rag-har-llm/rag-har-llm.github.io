<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta name="description" content="RAG-HAR Ablation Study - Supplementary Materials" />
    <meta property="og:title" content="RAG-HAR: Ablation Study" />
    <meta property="og:url" content="https://rag-har-llm.github.io/ablation" />

    <title>RAG-HAR:Ablation Study</title>
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="/static/css/bulma.min.css" />
    <link rel="stylesheet" href="/static/css/index.css" />

    <style>
      .content h1 { font-family: 'Google Sans', sans-serif; font-size: 2.5rem; margin-bottom: 1.5rem; }
      .content h2 { font-family: 'Google Sans', sans-serif; font-size: 1.75rem; margin-top: 2.5rem; margin-bottom: 1rem; }
      .content h3 { font-family: 'Google Sans', sans-serif; font-size: 1.4rem; margin-top: 2rem; margin-bottom: 0.75rem; }
      .content h4 { font-family: 'Google Sans', sans-serif; font-size: 1.2rem; margin-top: 1.5rem; margin-bottom: 0.5rem; color: #555; }
      .content h5 { font-family: 'Google Sans', sans-serif; font-size: 1rem; margin-top: 1.5rem; margin-bottom: 0.5rem; font-style: italic; }
      .content p { margin-bottom: 1rem; line-height: 1.7; }
      .content ul { margin-left: 1.5rem; margin-bottom: 1rem; }
      .content li { margin-bottom: 0.5rem; }
      .content table { width: 100%; margin: 1.5rem 0; border-collapse: collapse; }
      .content th, .content td { padding: 0.75rem 1rem; text-align: left; border: 1px solid #ddd; }
      .content th { background-color: #f5f5f5; font-weight: 600; }
      .content hr { margin: 2rem 0; border: none; border-top: 1px solid #ddd; }
      .back-link { margin-bottom: 2rem; }
      .back-link a { color: hsl(204, 86%, 53%); text-decoration: none; }
      .back-link a:hover { text-decoration: underline; }
    </style>
  </head>
  <body>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="back-link">
          <a href="/">&larr; Back to RAG-HAR</a>
        </div>
        <div class="content">
          <h1>Ablation Study</h1>

          <h2>Experimental Setup</h2>
          <p>We use publicly available and widely adopted HAR datasets for our experiments.</p>

          <h4>HHAR</h4>
          <p><em>Source: <a href="https://archive.ics.uci.edu/dataset/344/heterogeneity+activity+recognition" target="_blank">UCI Repository</a></em></p>
          <p>This dataset primarily contains locomotion-style activities such as walking, sitting, standing, and going up or down the stairs. The data was collected from smartphones and smartwatches placed on the wrist, consisting of 6 channels. We use 2-second windows with 50% overlap.</p>

          <h4>PAMAP2</h4>
          <p><em>Source: <a href="https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring" target="_blank">UCI Repository</a></em></p>
          <p>This dataset includes 12 protocol-defined activities recorded from 8 subjects. We retain 36 IMU channels, excluding heart rate, temperature, and orientation. Data is sampled at 100 Hz, with 2-second windows and a 0.5-second step. The dataset covers locomotion as well as daily activities such as ironing and vacuum cleaning.</p>

          <h4>MHEALTH</h4>
          <p><em>Source: <a href="https://archive.ics.uci.edu/dataset/319/mhealth+dataset" target="_blank">UCI Repository</a></em></p>
          <p>This dataset contains 12 activities, including locomotion and exercises such as jogging and cycling. It consists of 21 channels sampled at 50 Hz. We use sliding windows of 4 seconds with a 1-second step.</p>

          <h4>GOTOV</h4>
          <p><em>Source: <a href="https://data.4tu.nl/datasets/f9bae0cd-ec4e-4cfb-aaa5-41bd1c5554ce/1" target="_blank">4TU Research Data</a></em></p>
          <p>This dataset includes 16 activities collected from older adults (aged 61+) across three body positions, with 9 sensor channels in total. It contains pace variations (e.g., walking slow, normal, fast) and equipment variations (e.g., sitting on chair, sofa, or couch). Windows are created using 24-sample segments with 50% overlap.</p>

          <h4>SKODA</h4>
          <p><em>Source: <a href="https://drive.google.com/file/d/15Q8oV02h2_e94IWJ9rnKLrSCKPCTW5FS/view?usp=drive_link" target="_blank">Data Repository</a></em></p>
          <p>This dataset records 10 assembly-line activities performed by car manufacturing workers using 60 sensors on the right-hand side of the body. We use 24-sample windows with 50% overlap.</p>

          <h4>USC-HAD</h4>
          <p><em>Source: <a href="https://sipi.usc.edu/had/" target="_blank">USC</a></em></p>
          <p>This dataset contains 12 activities collected from 14 subjects using accelerometer and gyroscope signals (6 channels). Data is downsampled to 33.3 Hz and segmented into 1-second windows with 50% overlap. Subjects 13 and 14 are reserved for testing.</p>

          <br />
          <h2>Temporal and Hyperparameter Analysis of RAG-HAR</h2>
          <p>This analysis was conducted on the MHEALTH dataset.</p>

          <h3>Temporal Partitioning Analysis</h3>
          <p>We investigated the effect of different temporal partitioning strategies used during hybrid search. The motivation for this analysis is that activity patterns may not be uniformly distributed across the temporal dimension of a signal window. Transitions often occur at the beginning or end of a segment, while steady-state motion is captured in the middle portion. By explicitly modeling different temporal partitions, we aim to assess whether combining localized representations yields better retrieval performance compared to a single global segments.</p>

          <p>We considered the following partitioning configurations:</p>
          <ul>
            <li><strong>Full segment only:</strong> A single embedding computed over the entire temporal window.</li>
            <li><strong>Start + End sub-segments:</strong> Separate embeddings computed from the initial and final sub-segments of the window.</li>
            <li><strong>Start + Mid + End sub-segments:</strong> Embeddings derived from three equal sub-segments, excluding the whole-window representation.</li>
            <li><strong>Full segment + Start + Mid + End sub-segments:</strong> A hybrid configuration that combines the global representation with localized embeddings from all three sub-segments.</li>
          </ul>

          <p>The results are summarized in <strong>Table 1</strong>. The full segment representation alone provides a strong baseline. Incorporating start and end sub-segments captures transitional dynamics, yielding modest gains. Adding mid-segment information further improves performance, particularly for activities with sustained motion. The best results are consistently obtained when both the global embedding and all local sub-segments are combined, demonstrating that global context and localized temporal cues are complementary.</p>

          <h5>Table 1. Impact of different temporal partitioning strategies on retrieval performance</h5>
          <table>
            <thead>
              <tr>
                <th>Partitioning Strategy</th>
                <th>Accuracy</th>
                <th>F1-score</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>Full segment only</td><td>92.00</td><td>92.13</td></tr>
              <tr><td>Start + End</td><td>91.00</td><td>91.86</td></tr>
              <tr><td>Start + Mid + End</td><td>90.00</td><td>90.79</td></tr>
              <tr><td>Full + Start + Mid + End</td><td>95.00</td><td>94.90</td></tr>
            </tbody>
          </table>

          <hr />

          <h3>Hyperparameter Sensitivity Analysis</h3>
          <p>To better understand the impact of different configurations, we conducted a systematic hyperparameter sensitivity analysis on the MHEALTH dataset. The goal was to isolate the contribution of each parameter while controlling for confounding effects. Following are the models and hyper-parameters used as our <strong>default configuration</strong> throughout the experiments:</p>
          <ul>
            <li>Classifier model: <code>gpt-5-mini</code></li>
            <li>Embedding model: <code>text-embedding-3-small</code></li>
            <li>Weighted Reranking: (0.40, 0.20, 0.20, 0.20)</li>
            <li>Retrieved context: 10</li>
          </ul>
          <p>When exploring the effect of one parameter, all others were fixed at these default values. This controlled setup allows us to attribute performance differences directly to the varied hyperparameter. The analysis was conducted on a subset of 100 samples, selected to maintain a balanced distribution of activity classes while ensuring the feasibility of repeated sensitivity experiments within reasonable computational limits.</p>

          <hr />

          <h4>Hybrid Search Weights</h4>
          <p>We studied the effect of different weighting strategies in the hybrid search mechanism, which combines full-segment, start, mid, and end sub-segment embeddings. The results are presented in <strong>Table 2</strong>. We find that the scheme <code>[0.4, 0.2, 0.2, 0.2]</code> achieves the highest accuracy. By assigning greater weight to the full-segment embedding, this configuration allows the hybrid search to prioritize global activity patterns while still incorporating complementary fine-grained cues from localized partitions.</p>

          <h5>Table 2. Impact of hybrid search weighting on retrieval accuracy and F1-score</h5>
          <table>
            <thead>
              <tr>
                <th>Weights [full-segment, start, mid, end]</th>
                <th>Accuracy</th>
                <th>F1-score</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>[0.25, 0.25, 0.25, 0.25]</td><td>93.00</td><td>93.90</td></tr>
              <tr><td>[0.4, 0.2, 0.2, 0.2]</td><td>95.00</td><td>94.90</td></tr>
              <tr><td>[0.1, 0.4, 0.1, 0.4]</td><td>92.50</td><td>92.40</td></tr>
              <tr><td>[0.2, 0.2, 0.4, 0.2]</td><td>85.00</td><td>85.30</td></tr>
            </tbody>
          </table>

          <hr />

          <h4>Embedding Models</h4>
          <p>Next, we compared different embedding models for vectorization. The results are presented in <strong>Table 3</strong>. <code>text-embedding-3-small</code> achieved strong and comparable performance, while <code>text-embedding-3-large</code> underperformed. This suggests that compact embedding models can be more effective than higher-capacity variants for retrieval-augmented classification, as they often produce cleaner, more generalizable similarity representations, whereas larger models may introduce noise or overfit to less relevant details.</p>

          <h5>Table 3. Impact of different embedding models on retrieval accuracy and F1-score</h5>
          <table>
            <thead>
              <tr>
                <th>Text Embedding Model</th>
                <th>Accuracy</th>
                <th>F1-Score</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>text-embedding-3-small</td><td>95.0</td><td>94.9</td></tr>
              <tr><td>text-embedding-3-large</td><td>89.0</td><td>89.6</td></tr>
              <tr><td>Ada-002</td><td>85.0</td><td>82.2</td></tr>
            </tbody>
          </table>

          <hr />

          <h4>Retrieved-Context Exemplars</h4>
          <p>We then varied the number of retrieved-context exemplars provided in the prompt. The results are presented in <strong>Table 4</strong>. We found that performance increases from 5 to 10 exemplars, with 10 examples achieving the best results. However, increasing the number beyond 10 led to a slight decrease in accuracy, suggesting that adding more context does not necessarily improve performance and may even dilute the most relevant signals. This indicates that relatively small exemplar settings are sufficient for strong performance.</p>

          <h5>Table 4. Impact of number of retrieved-context exemplars on classification performance</h5>
          <table>
            <thead>
              <tr>
                <th>Number of Exemplars</th>
                <th>Accuracy</th>
                <th>F1-Score</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>5</td><td>90.8</td><td>91.0</td></tr>
              <tr><td>10</td><td>95.0</td><td>95.0</td></tr>
              <tr><td>15</td><td>94.0</td><td>92.4</td></tr>
              <tr><td>20</td><td>95.0</td><td>94.45</td></tr>
              <tr><td>25</td><td>92.0</td><td>90.33</td></tr>
            </tbody>
          </table>

          <hr />

          <h4>Classifier Model</h4>
          <p>We evaluated the impact of different large language models while keeping embeddings and search configurations fixed. The results are presented in <strong>Table 5</strong>. The reasoning-oriented <code>gpt-5-mini</code> achieved the best performance (96.91% accuracy, 96.7% F1), confirming the advantage of specialized reasoning models in structured classification tasks. The open-source <code>gpt-oss-20b</code> delivered moderate results (81.0% accuracy, 82.9% F1), while the larger instruction-tuned models <code>gemma-27b-it</code> (90.0% accuracy, 89.6% F1) and <code>llama3.3</code> (89.0% accuracy, 88.3% F1) achieved higher accuracy. The general-purpose <code>gpt-4o</code> achieved 88.0% accuracy and 87.6% F1.</p>

          <h5>Table 5. Impact of different LLM models on classification performance</h5>
          <table>
            <thead>
              <tr>
                <th>LLM Model</th>
                <th>Accuracy</th>
                <th>F1-Score</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>gpt-5-mini</td><td>96.91</td><td>96.7</td></tr>
              <tr><td>gpt-4o</td><td>88.0</td><td>87.6</td></tr>
              <tr><td>llama3.3</td><td>89.0</td><td>88.3</td></tr>
              <tr><td>gemma-27b-it</td><td>90.0</td><td>89.6</td></tr>
              <tr><td>gpt-oss-20b</td><td>81.0</td><td>82.9</td></tr>
            </tbody>
          </table>

          <hr />

          <h4>Prompt Optimization</h4>
          <p>To further improve classification performance, we conducted prompt optimization experiments on selected models. By refining the prompt structure and instructions, we achieved notable performance gains. The results are presented in <strong>Table 6</strong>. After optimization, <code>gpt-5-mini</code> reached 98.7% accuracy. The lower-performing <code>gpt-oss-20b</code> improved substantially from 81.0% to 91.0% accuracy. We also observed a modest gain for <code>gemma-27b-it</code>, which increased from 90.0% to 91.0% accuracy after prompt tuning. These results show that prompt optimization can benefit both proprietary and open-source models, with the largest relative gains often appearing on models that started with lower baseline performance.</p>

          <h5>Table 6. Impact of prompt optimization on classification performance</h5>
          <table>
            <thead>
              <tr>
                <th>LLM Model</th>
                <th>Accuracy (Before)</th>
                <th>Accuracy (After)</th>
                <th>Improvement</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>gpt-5-mini</td><td>96.91</td><td>98.7</td><td>+1.79</td></tr>
              <tr><td>gpt-oss-20b</td><td>81.0</td><td>91.0</td><td>+10.0</td></tr>
              <tr><td>gemma-27b-it</td><td>90.0</td><td>94.0</td><td>+4.0</td></tr>
            </tbody>
          </table>

        </div>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content has-text-centered">
              <p>
                <a href="/">Back to RAG-HAR main page</a>
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
